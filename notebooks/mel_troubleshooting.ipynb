{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa as lr\n",
    "import pescador\n",
    "from tqdm import tqdm\n",
    "import keras as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.backend import squeeze\n",
    "import json\n",
    "import six\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import ast\n",
    "from keras.layers import Input\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/ci411/pcen-t-varying/')\n",
    "\n",
    "from pcen_t.utils import *\n",
    "from pcen_t.models import MODELS\n",
    "from pcen_t.pcen_pump import *\n",
    "\n",
    "URBANSED_CLASSES = ['air_conditioner',\n",
    "                    'car_horn',\n",
    "                    'children_playing',\n",
    "                    'dog_bark',\n",
    "                    'drilling',\n",
    "                    'engine_idling',\n",
    "                    'gun_shot',\n",
    "                    'jackhammer',\n",
    "                    'siren',\n",
    "                    'street_music']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sampler(max_samples, duration, pump, seed):\n",
    "    op = pump.ops[0].name\n",
    "    n_frames = lr.time_to_frames(duration,\n",
    "                                 sr=pump[op].sr,\n",
    "                                 hop_length=pump[op].hop_length)\n",
    "\n",
    "    return pump.sampler(max_samples, n_frames, random_state=seed)\n",
    "\n",
    "\n",
    "@pescador.streamable\n",
    "def data_sampler(fname, sampler, slices):\n",
    "    '''Generate samples from a specified h5 file'''\n",
    "    data_dict = load_h5(fname)\n",
    "    field = list(pump.fields.keys())[0]\n",
    "    if slices is not None:\n",
    "        data_dict[field] = data_dict[field][:,:,:,slices]\n",
    "    file_sampler = sampler(data_dict)\n",
    "    for datum in file_sampler:\n",
    "        yield datum            \n",
    "\n",
    "    \n",
    "def data_generator(directories, sampler, k, rate, batch_size=32, slices=None, **kwargs):\n",
    "    '''Generate a data stream from a collection of tracks and a sampler'''\n",
    "\n",
    "    seeds = []\n",
    "    for working in directories:\n",
    "        for track in tqdm(find_files(working,ext='h5')):\n",
    "            fname = os.path.join(working,track)\n",
    "            seeds.append(data_sampler(fname, sampler, slices))\n",
    "\n",
    "    # Send it all to a mux\n",
    "    mux = pescador.StochasticMux(seeds, k, rate, mode='with_replacement', **kwargs)\n",
    "\n",
    "    return mux\n",
    "\n",
    "def data_generator_val(directories, sampler, batch_size=32, slices=None, **kwargs):\n",
    "    '''Generate a data stream from a collection of tracks and a sampler'''\n",
    "\n",
    "    seeds = []\n",
    "    for working in directories:\n",
    "        for track in tqdm(find_files(working,ext='h5')):\n",
    "            fname = os.path.join(working,track)\n",
    "            seeds.append(data_sampler(fname, sampler, slices))\n",
    "\n",
    "    # Send it all to a mux\n",
    "    mux = pescador.ChainMux(seeds, mode='cycle', **kwargs)\n",
    "\n",
    "    return mux\n",
    "   \n",
    "    \n",
    "def keras_tuples(gen, inputs=None, outputs=None):\n",
    "\n",
    "    if isinstance(inputs, six.string_types):\n",
    "        if isinstance(outputs, six.string_types):\n",
    "            # One input, one output\n",
    "            for datum in gen:\n",
    "                yield (datum[inputs], datum[outputs])\n",
    "        else:\n",
    "            # One input, multi outputs\n",
    "            for datum in gen:\n",
    "                yield (datum[inputs], [datum[o] for o in outputs])\n",
    "    else:\n",
    "        if isinstance(outputs, six.string_types):\n",
    "            for datum in gen:\n",
    "                yield ([datum[i] for i in inputs], datum[outputs])\n",
    "        else:\n",
    "            # One input, multi outputs\n",
    "            for datum in gen:\n",
    "                yield ([datum[i] for i in inputs],\n",
    "                       [datum[o] for o in outputs])\n",
    "\n",
    "def label_transformer_generator(generator):\n",
    "    for data in generator:\n",
    "        features, labels = data\n",
    "        yield (features, max_pool(labels[0]))\n",
    "        \n",
    "        \n",
    "class LossHistory(K.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, outfile):\n",
    "        super().__init__()\n",
    "        self.outfile = outfile\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.loss.append(logs.get('loss'))\n",
    "        self.val_loss.append(logs.get('val_loss'))\n",
    "\n",
    "        loss_dict = {'loss': self.loss, 'val_loss': self.val_loss}\n",
    "        with open(self.outfile, 'wb+') as fp:\n",
    "            pickle.dump(loss_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading URBAN-SED_dry: \t/beegfs/ci411/pcen/features_807/mel/URBAN-SED_dry/train \t/beegfs/ci411/pcen/features_807/unpitched/mel/URBAN-SED_dry/validate\n",
      "Loading URBAN-SED_sim_short: \t/beegfs/ci411/pcen/features_807/mel/URBAN-SED_sim_short/train \t/beegfs/ci411/pcen/features_807/unpitched/mel/URBAN-SED_sim_short/validate\n",
      "Loading URBAN-SED_sim_medium: \t/beegfs/ci411/pcen/features_807/mel/URBAN-SED_sim_medium/train \t/beegfs/ci411/pcen/features_807/unpitched/mel/URBAN-SED_sim_medium/validate\n",
      "Loading URBAN-SED_sim_long: \t/beegfs/ci411/pcen/features_807/mel/URBAN-SED_sim_long/train \t/beegfs/ci411/pcen/features_807/unpitched/mel/URBAN-SED_sim_long/validate\n",
      "Loading URBAN-SED_alley: \t/beegfs/ci411/pcen/features_807/mel/URBAN-SED_alley/train \t/beegfs/ci411/pcen/features_807/unpitched/mel/URBAN-SED_alley/validate\n",
      "Loading URBAN-SED_bedroom: \t/beegfs/ci411/pcen/features_807/mel/URBAN-SED_bedroom/train \t/beegfs/ci411/pcen/features_807/unpitched/mel/URBAN-SED_bedroom/validate\n",
      "Loading URBAN-SED_tunnel: \t/beegfs/ci411/pcen/features_807/mel/URBAN-SED_tunnel/train \t/beegfs/ci411/pcen/features_807/unpitched/mel/URBAN-SED_tunnel/validate\n"
     ]
    }
   ],
   "source": [
    "#get feature paths\n",
    "\n",
    "mel_path = \"/beegfs/ci411/pcen/features_807/mel\"\n",
    "mel_dest_path = \"/beegfs/ci411/pcen/features_807/unpitched/mel\"\n",
    "\n",
    "\n",
    "train_features = []\n",
    "valid_features = []\n",
    "feature_list = ['URBAN-SED_dry','URBAN-SED_sim_short','URBAN-SED_sim_medium','URBAN-SED_sim_long','URBAN-SED_alley','URBAN-SED_bedroom','URBAN-SED_tunnel']\n",
    "for feature_name in feature_list:\n",
    "    train_features.append(os.path.join(mel_path, feature_name, 'train'))\n",
    "    valid_features.append(os.path.join(mel_dest_path, feature_name, 'validate'))\n",
    "    print('Loading {}: \\t{} \\t{}'.format(feature_name, train_features[-1], valid_features[-1]))\n",
    "pump = load_pump(os.path.join('/beegfs/ci411/pcen/pumps/mel', 'pump.pkl'))\n",
    "field = list(pump.fields.keys())[0]\n",
    "sampler = make_sampler(128, 10.0, pump, 20170613)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:00<00:00, 297714.70it/s]\n",
      "100%|██████████| 30000/30000 [00:00<00:00, 296685.62it/s]\n",
      "100%|██████████| 30000/30000 [00:00<00:00, 74420.18it/s]\n",
      "100%|██████████| 30000/30000 [00:00<00:00, 296589.12it/s]\n",
      "100%|██████████| 30000/30000 [00:00<00:00, 294189.17it/s]\n",
      "100%|██████████| 30000/30000 [00:00<00:00, 65307.15it/s]\n",
      "100%|██████████| 30000/30000 [00:00<00:00, 296556.27it/s]\n",
      "100%|██████████| 30000/30000 [00:00<00:00, 279645.75it/s]\n",
      "100%|██████████| 30000/30000 [00:00<00:00, 294408.06it/s]\n",
      "100%|██████████| 30000/30000 [00:00<00:00, 72230.16it/s]\n",
      "100%|██████████| 30000/30000 [00:00<00:00, 292756.58it/s]\n",
      "100%|██████████| 30000/30000 [00:00<00:00, 291722.88it/s]\n",
      "100%|██████████| 30000/30000 [00:00<00:00, 67935.69it/s]\n",
      "100%|██████████| 30000/30000 [00:00<00:00, 299377.16it/s]\n"
     ]
    }
   ],
   "source": [
    "sampler_val = make_sampler(1, 10.0, pump, 20170613)\n",
    "\n",
    "slices = None\n",
    "\n",
    "#define model, inputs, and outputs\n",
    "construct_model = MODELS['cnnl3_7_strong']\n",
    "\n",
    "if slices is not None:\n",
    "    input_layer = Input(name=field,  shape=(None, 128, len(slices)),\\\n",
    "                              dtype='float32')    \n",
    "else:\n",
    "    input_layer = Input(name=field,  shape=(None, 128, 1),\\\n",
    "                              dtype='float32')  \n",
    "\n",
    "model, inputs, outputs = construct_model(input_layer, pump)    \n",
    "\n",
    "\n",
    "output_vars = 'dynamic/tags'\n",
    "\n",
    "#create streanable generators\n",
    "gen_train = data_generator(train_features, sampler, 64,\\\n",
    "                       4, random_state=20170613, slices=slices)\n",
    "\n",
    "gen_val = data_generator_val(train_features, sampler_val, random_state=20170613, slices=slices)\n",
    "\n",
    "#convert to keras tuples\n",
    "gen_train = keras_tuples(gen_train(), inputs=inputs, outputs=output_vars)\n",
    "gen_val = keras_tuples(gen_val(), inputs=inputs, outputs=output_vars)\n",
    "\n",
    "#apply label transformation\n",
    "gen_train_label = label_transformer_generator(gen_train)\n",
    "gen_val_label = label_transformer_generator(gen_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 215, 128, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_val_label.__next__()[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model...\n",
      "Epoch 1/150\n",
      "510/512 [============================>.] - ETA: 0s - loss: 0.8174 - accuracy: 0.5332"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-6a19a6f9ab20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mval_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m history = model.fit_generator(gen_train_label, 512, 150,\n\u001b[0m\u001b[1;32m     43\u001b[0m                               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen_val_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                               verbose=verbosity, callbacks=cb, max_queue_size=16)\n",
      "\u001b[0;32m~/.conda/envs/py3.8/lib/python3.8/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py3.8/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1716\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1717\u001b[0m         \"\"\"\n\u001b[0;32m-> 1718\u001b[0;31m         return training_generator.fit_generator(\n\u001b[0m\u001b[1;32m   1719\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1720\u001b[0m             \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py3.8/lib/python3.8/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    236\u001b[0m                     \u001b[0;31m# `keras.callbacks.CallbackList`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mval_gen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m                         val_outs = model.evaluate_generator(\n\u001b[0m\u001b[1;32m    239\u001b[0m                             \u001b[0mval_enqueuer_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                             \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py3.8/lib/python3.8/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py3.8/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, steps, callbacks, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0man\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m         \"\"\"\n\u001b[0;32m-> 1784\u001b[0;31m         return training_generator.evaluate_generator(\n\u001b[0m\u001b[1;32m   1785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m             \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py3.8/lib/python3.8/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(model, generator, steps, callbacks, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msteps_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             outs = model.test_on_batch(x, y,\n\u001b[0m\u001b[1;32m    400\u001b[0m                                        \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m                                        reset_metrics=False)\n",
      "\u001b[0;32m~/.conda/envs/py3.8/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[0;34m(self, x, y, sample_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1557\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1558\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1559\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py3.8/lib/python3.8/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.conda/envs/py3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py3.8/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#establish model definitions\n",
    "loss = {output_vars: 'binary_crossentropy'}\n",
    "metrics = {output_vars: 'accuracy'}\n",
    "monitor = 'val_{}_acc'.format(output_vars)\n",
    "\n",
    "model.compile(K.optimizers.Adam(learning_rate=1e-5), loss=loss, metrics=metrics)\n",
    "\n",
    "# Construct the weight path\n",
    "weight_path = os.path.join('/beegfs/ci411/pcen/models/meltest', 'testmodel', 'model.h5')\n",
    "\n",
    "# Build the callbacks\n",
    "cb = []\n",
    "cb.append(K.callbacks.ModelCheckpoint(weight_path,\n",
    "                                      save_best_only=True,\n",
    "                                      verbose=1,\n",
    "                                      monitor=monitor))\n",
    "\n",
    "cb.append(K.callbacks.ReduceLROnPlateau(patience=10,\n",
    "                                        verbose=1,\n",
    "                                        monitor=monitor))\n",
    "\n",
    "cb.append(K.callbacks.EarlyStopping(patience=30,\n",
    "                                    verbose=1,\n",
    "                                    monitor=monitor))\n",
    "\n",
    "history_checkpoint = os.path.join('/beegfs/ci411/pcen/models/meltest', 'testmodel',\n",
    "                                  'history_checkpoint.pkl')\n",
    "cb.append(LossHistory(history_checkpoint))\n",
    "\n",
    "history_csvlog = os.path.join('/beegfs/ci411/pcen/models/meltest', 'testmodel', 'history_csvlog.csv')\n",
    "cb.append(K.callbacks.CSVLogger(history_csvlog, append=True,\n",
    "                                separator=','))\n",
    "\n",
    "print('Fit model...')\n",
    "if True:\n",
    "    verbosity = 1\n",
    "else:\n",
    "    verbosity = 2\n",
    "\n",
    "    \n",
    "val_size = 1000 *6*5* len(feature_list)\n",
    "history = model.fit_generator(gen_train_label, 512, 150,\n",
    "                              validation_data=gen_val_label, validation_steps=val_size,\n",
    "                              verbose=verbosity, callbacks=cb, max_queue_size=16)\n",
    "\n",
    "#make or clear output directory\n",
    "make_dirs(os.path.join('/beegfs/ci411/pcen/models/meltest', 'testmodel'))\n",
    "\n",
    "# Store the model\n",
    "# save the model object\n",
    "model_spec = K.utils.serialize_keras_object(model)\n",
    "with open(os.path.join('/beegfs/ci411/pcen/models/meltest', 'testmodel', 'model_spec.pkl'),\\\n",
    "          'wb') as fd:\n",
    "    pickle.dump(model_spec, fd)\n",
    "\n",
    "# save the model definition\n",
    "modelyamlfile = os.path.join('/beegfs/ci411/pcen/models/meltest', 'testmodel', 'model.yaml')\n",
    "model_yaml = model.to_yaml()\n",
    "with open(modelyamlfile, 'w') as yaml_file:\n",
    "    yaml_file.write(model_yaml)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Done training. Saving results to disk...')\n",
    "# Save history\n",
    "with open(os.path.join('/beegfs/ci411/pcen/models/meltest', 'testmodel', 'history.pkl'), 'wb') as fd:\n",
    "    pickle.dump(history.history, fd)\n",
    "print('Saving Weights')\n",
    "model.save_weights(weight_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
