{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa as lr\n",
    "import pescador\n",
    "from tqdm import tqdm\n",
    "import keras as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.backend import squeeze\n",
    "import json\n",
    "import six\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import ast\n",
    "from keras.layers import Input\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/ci411/pcen-t-varying/')\n",
    "\n",
    "from pcen_t.utils import *\n",
    "from pcen_t.models import MODELS\n",
    "from pcen_t.pcen_pump import *\n",
    "\n",
    "URBANSED_CLASSES = ['air_conditioner',\n",
    "                    'car_horn',\n",
    "                    'children_playing',\n",
    "                    'dog_bark',\n",
    "                    'drilling',\n",
    "                    'engine_idling',\n",
    "                    'gun_shot',\n",
    "                    'jackhammer',\n",
    "                    'siren',\n",
    "                    'street_music']\n",
    "\n",
    "#check GPU usage\n",
    "import tensorflow as tf\n",
    "tf.config.list_logical_devices()\n",
    "\n",
    "def make_sampler(max_samples, duration, pump, seed):\n",
    "    op = pump.ops[0].name\n",
    "    n_frames = lr.time_to_frames(duration,\n",
    "                                 sr=pump[op].sr,\n",
    "                                 hop_length=pump[op].hop_length)\n",
    "    '''MODIFIED in model fix branch'''\n",
    "    n_frames = (n_frames // 16) * 16\n",
    "\n",
    "    return pump.sampler(max_samples, n_frames, random_state=seed)\n",
    "\n",
    "\n",
    "@pescador.streamable\n",
    "def data_sampler(fname, sampler, slices):\n",
    "    '''Generate samples from a specified h5 file'''\n",
    "    data_dict = load_h5(fname)\n",
    "    field = list(pump.fields.keys())[0]\n",
    "    if slices is not None:\n",
    "        data_dict[field] = data_dict[field][:,:,:,slices]\n",
    "    file_sampler = sampler(data_dict)\n",
    "    for datum in file_sampler:\n",
    "        yield datum            \n",
    "\n",
    "    \n",
    "def data_generator(directories, sampler, k, rate, batch_size=32, slices=None, **kwargs):\n",
    "    '''Generate a data stream from a collection of tracks and a sampler'''\n",
    "\n",
    "    seeds = []\n",
    "    for working in directories:\n",
    "        for track in find_files(working,ext='h5'):\n",
    "            fname = os.path.join(working,track)\n",
    "            seeds.append(data_sampler(fname, sampler, slices))\n",
    "    # Send it all to a mux\n",
    "    mux = pescador.StochasticMux(seeds, k, rate, mode='with_replacement', **kwargs)\n",
    "\n",
    "    return mux\n",
    "\n",
    "def data_generator_val(directories, sampler, batch_size=32, slices=None, **kwargs):\n",
    "    '''Generate a data stream from a collection of tracks and a sampler'''\n",
    "\n",
    "    seeds = []\n",
    "    total_files = 0\n",
    "    for working in directories:\n",
    "        for track in find_files(working,ext='h5'):\n",
    "            fname = os.path.join(working,track)\n",
    "            seeds.append(data_sampler(fname, sampler, slices))\n",
    "       \n",
    "    print(\"total files: {}\".format(total_files))\n",
    "    # Send it all to a mux\n",
    "    mux = pescador.ChainMux(seeds, mode='cycle', **kwargs)\n",
    "\n",
    "    return mux\n",
    "   \n",
    "    \n",
    "def keras_tuples(gen, inputs=None, outputs=None):\n",
    "    for datum in gen:\n",
    "        yield (datum[inputs], datum[outputs])\n",
    "\n",
    "def label_transformer_generator(generator):\n",
    "    for data in generator:\n",
    "        features, labels = data\n",
    "        yield (features, labels)#max_pool(labels))\n",
    "        \n",
    "        \n",
    "class LossHistory(K.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, outfile):\n",
    "        super().__init__()\n",
    "        self.outfile = outfile\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.loss.append(logs.get('loss'))\n",
    "        self.val_loss.append(logs.get('val_loss'))\n",
    "\n",
    "        loss_dict = {'loss': self.loss, 'val_loss': self.val_loss}\n",
    "        with open(self.outfile, 'wb+') as fp:\n",
    "            pickle.dump(loss_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_arguments(args):\n",
    "    parser = argparse.ArgumentParser(description=__doc__)\n",
    "\n",
    "    parser.add_argument('--slices', dest='slices', type=str,\n",
    "                        default=None,\n",
    "                        help='Slices to keep for training')\n",
    "    \n",
    "    parser.add_argument('--nmels', dest='n_mels', type=float, default=128,\n",
    "                        help='Number of bins in Mel Spectrogram')\n",
    "    \n",
    "    parser.add_argument('--max_samples', dest='max_samples', type=int,\n",
    "                        default=128,\n",
    "                        help='Maximum number of samples to draw per streamer')\n",
    "\n",
    "    parser.add_argument('--patch-duration', dest='duration', type=float,\n",
    "                        default=10.0,\n",
    "                        help='Duration (in seconds) of training patches')\n",
    "\n",
    "    parser.add_argument('--seed', dest='seed', type=int,\n",
    "                        default='20170613',\n",
    "                        help='Seed for the random number generator')\n",
    "\n",
    "    parser.add_argument('--train-streamers', dest='train_streamers', type=int,\n",
    "                        default=64,\n",
    "                        help='Number of active training streamers')\n",
    "\n",
    "    parser.add_argument('--batch-size', dest='batch_size', type=int,\n",
    "                        default=16,\n",
    "                        help='Size of training batches')\n",
    "\n",
    "    parser.add_argument('--rate', dest='rate', type=int,\n",
    "                        default=4,\n",
    "                        help='Rate of pescador stream deactivation')\n",
    "\n",
    "    parser.add_argument('--epochs', dest='epochs', type=int,\n",
    "                        default=150,\n",
    "                        help='Maximum number of epochs to train for')\n",
    "\n",
    "    parser.add_argument('--epoch-size', dest='epoch_size', type=int,\n",
    "                        default=512,\n",
    "                        help='Number of batches per epoch')\n",
    "\n",
    "    parser.add_argument('--early-stopping', dest='early_stopping', type=int,\n",
    "                        default=30,\n",
    "                        help='# epochs without improvement to stop')\n",
    "    \n",
    "    parser.add_argument('--learning-rate', dest='learning_rate', type=float,\n",
    "                        default=1e-5,\n",
    "                        help='# Learning rate of the optimizer')\n",
    "\n",
    "    parser.add_argument('--reduce-lr', dest='reduce_lr', type=int,\n",
    "                        default=10,\n",
    "                        help='# epochs before reducing learning rate')\n",
    "    \n",
    "    parser.add_argument('--validation-size', dest='validation_size', type=int,\n",
    "                        default=2000,\n",
    "                        help='validation steps per feature (per epoch)')\n",
    "\n",
    "    parser.add_argument('--verbose', dest='verbose', action='store_const',\n",
    "                        const=True, default=False,\n",
    "                        help='Call keras fit with verbose mode (1)')\n",
    "\n",
    "    parser.add_argument('--model-name', dest='modelname', type=str,\n",
    "                        default='cnnl3_7_strong',\n",
    "                        help='Name of model to train')\n",
    "\n",
    "    parser.add_argument('--model-id', dest='modelid', type=str,\n",
    "                        default='model_test',\n",
    "                        help='Model ID number, e.g. \"model001\"')\n",
    "    \n",
    "    parser.add_argument('--training-dir', dest='training_dir', type=str,\n",
    "                        default='/beegfs/ci411/pcen/features_pitched/pcen',\n",
    "                        help='Location to load features for training')\n",
    "    \n",
    "    parser.add_argument('--validation-dir', dest='validation_dir', type=str,\n",
    "                        default='/beegfs/ci411/pcen/features/pcen',\n",
    "                        help='Location to load features for validation')\n",
    "    \n",
    "    parser.add_argument('--feature-names', dest='feature_names', type=str,\n",
    "                        default='[\"URBAN-SED_dry\"]',\n",
    "                        help='Names of feature directories to load')\n",
    "    \n",
    "    parser.add_argument('--model-dir', dest='model_dir', type=str,\n",
    "                        default='/beegfs/ci411/pcen/models',\n",
    "                        help='Location to store models and weights')\n",
    "    \n",
    "    parser.add_argument('--load-pump', dest='load_pump', type=str,\n",
    "                        default='/beegfs/ci411/pcen/pumps/pcen/',\n",
    "                        help='Directory containing pump file')\n",
    "    \n",
    "\n",
    "    return parser.parse_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Features...\n",
      "Loading URBAN-SED_dry, \t/beegfs/ci411/pcen/features_807/mel/URBAN-SED_dry/train \t/beegfs/ci411/pcen/features_807/unpitched/mel/URBAN-SED_dry/validate\n"
     ]
    }
   ],
   "source": [
    "training_dir=\"/beegfs/ci411/pcen/features_807/mel\"\n",
    "validation_dir=\"/beegfs/ci411/pcen/features_807/unpitched/mel\"\n",
    "pump_dir='/beegfs/ci411/pcen/pumps/mel'\n",
    "\n",
    "#params = process_arguments(sys.argv[1:])\n",
    "    \n",
    "print(\"Loading Features...\")\n",
    "#get feature paths\n",
    "train_features = []\n",
    "valid_features = []\n",
    "#feature_list = ast.literal_eval(TRAINFEATURENAMES)#params.feature_names)\n",
    "feature_list=['URBAN-SED_dry']\n",
    "\n",
    "for feature_name in feature_list:\n",
    "    train_features.append(os.path.join(training_dir, feature_name, 'train'))\n",
    "    valid_features.append(os.path.join(validation_dir, feature_name, 'validate'))\n",
    "    print('Loading {}, \\t{} \\t{}'.format(feature_name, train_features[-1], valid_features[-1]))\n",
    "pump = load_pump(os.path.join(pump_dir, 'pump.pkl'))\n",
    "field = list(pump.fields.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing model...\n"
     ]
    }
   ],
   "source": [
    "slices = None\n",
    "n_mels = 128\n",
    "modelname = 'cnnl3_7_strong'\n",
    "print(\"Constructing model...\")\n",
    "construct_model = MODELS[modelname]\n",
    "\n",
    "if slices is not None:\n",
    "    input_layer = Input(name=field,  shape=(None, n_mels, len(slices)), dtype='float32')\n",
    "else:\n",
    "    input_layer = Input(name=field,  shape=(None, n_mels, 1), dtype='float32')\n",
    "model, inputs, outputs = construct_model(input_layer, pump)    \n",
    "\n",
    "output_vars = 'dynamic/tags'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/beegfs/ci411/pcen/features_807/mel/URBAN-SED_dry/train']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Samplers...\n",
      "total files: 0\n"
     ]
    }
   ],
   "source": [
    "max_samples = 128\n",
    "duration = 10.\n",
    "seed = 20201012\n",
    "validation_size = 2000\n",
    "train_streamers = 64\n",
    "rate = 4\n",
    "batch_size = 16\n",
    "\n",
    "print(\"Generating Samplers...\")\n",
    "sampler = make_sampler(max_samples, duration, pump, seed)\n",
    "\n",
    "val_size = validation_size * len(feature_list)\n",
    "sampler_val = make_sampler(1, duration, pump, seed)\n",
    "\n",
    "gen_train = data_generator(train_features, sampler, train_streamers,\\\n",
    "                           rate, random_state=seed, slices=slices,\\\n",
    "                           batch_size=batch_size)\n",
    "\n",
    "gen_val = data_generator_val(valid_features, sampler_val, random_state=seed, slices=slices,\\\n",
    "                             batch_size=batch_size)\n",
    "\n",
    "gen_train = keras_tuples(gen_train(), inputs=inputs, outputs=output_vars)\n",
    "\n",
    "gen_val = keras_tuples(gen_val(), inputs=inputs, outputs=output_vars)\n",
    "\n",
    "gen_train_label = label_transformer_generator(gen_train)\n",
    "gen_val_label = label_transformer_generator(gen_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model...\n"
     ]
    }
   ],
   "source": [
    "model_dir = '/beegfs/ci411/pcen/models/models_1012_rebuild_test'\n",
    "modelid='model_1012_rebuild_test'\n",
    "learning_rate = 1e-5\n",
    "reduce_lr = 10\n",
    "early_stopping = 30\n",
    "\n",
    "print(\"Compiling model...\")\n",
    "loss = {output_vars: 'binary_crossentropy'}\n",
    "metrics = {output_vars: 'accuracy'}\n",
    "monitor = 'val_accuracy'#'val_{}_acc'.format(output_vars)\n",
    "\n",
    "\n",
    "model.compile(K.optimizers.Adam(learning_rate=learning_rate), loss=loss, metrics=metrics)\n",
    "\n",
    "# Construct the weight path\n",
    "weight_path = os.path.join(model_dir, modelid, 'model.h5')\n",
    "\n",
    "# Build the callbacks\n",
    "cb = []\n",
    "cb.append(K.callbacks.ModelCheckpoint(weight_path,\n",
    "                                      save_best_only=True,\n",
    "                                      verbose=1,\n",
    "                                      monitor=monitor))\n",
    "\n",
    "cb.append(K.callbacks.ReduceLROnPlateau(patience=reduce_lr,\n",
    "                                        verbose=1,\n",
    "                                        monitor=monitor))\n",
    "\n",
    "cb.append(K.callbacks.EarlyStopping(patience=early_stopping,\n",
    "                                    verbose=1,\n",
    "                                    monitor=monitor))\n",
    "\n",
    "history_checkpoint = os.path.join(model_dir, modelid,\n",
    "                                  'history_checkpoint.pkl')\n",
    "cb.append(LossHistory(history_checkpoint))\n",
    "\n",
    "history_csvlog = os.path.join(model_dir, modelid, 'history_csvlog.csv')\n",
    "cb.append(K.callbacks.CSVLogger(history_csvlog, append=True,\n",
    "                                separator=','))\n",
    "\n",
    "if not os.path.isdir(os.path.join(model_dir, modelid)):\n",
    "    os.makedirs(os.path.join(model_dir, modelid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model...\n",
      "Epoch 1/150\n",
      "248/512 [=============>................] - ETA: 17s - loss: 0.8510 - accuracy: 0.5165"
     ]
    }
   ],
   "source": [
    "epoch_size = 512\n",
    "epochs = 150\n",
    "\n",
    "verbose = True\n",
    "print('Fit model...')\n",
    "if verbose:\n",
    "    verbosity = 1\n",
    "else:\n",
    "    verbosity = 2\n",
    "\n",
    "history = model.fit(gen_train_label, steps_per_epoch=epoch_size, epochs=epochs,\n",
    "                              validation_data=gen_val_label, validation_steps=val_size,\n",
    "                              verbose=verbosity, callbacks=cb, max_queue_size=16)\n",
    "\n",
    "#make or clear output directory\n",
    "make_dirs(os.path.join(model_dir, modelid))\n",
    "\n",
    "# Store the model\n",
    "# save the model object\n",
    "model_spec = K.utils.serialize_keras_object(model)\n",
    "with open(os.path.join(model_dir, modelid, 'model_spec.pkl'),\\\n",
    "          'wb') as fd:\n",
    "    pickle.dump(model_spec, fd)\n",
    "\n",
    "# save the model definition\n",
    "modelyamlfile = os.path.join(model_dir, modelid, 'model.yaml')\n",
    "model_yaml = model.to_yaml()\n",
    "with open(modelyamlfile, 'w') as yaml_file:\n",
    "    yaml_file.write(model_yaml)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Done training. Saving results to disk...')\n",
    "# Save history\n",
    "with open(os.path.join(model_dir, modelid, 'history.pkl'), 'wb') as fd:\n",
    "    pickle.dump(history.history, fd)\n",
    "print('Saving Weights')\n",
    "model.save_weights(weight_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_h5(filename, trim=862):\n",
    "    '''Load data from an hdf5 file created by `save_h5`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        Path to the hdf5 file\n",
    "    Returns\n",
    "    -------\n",
    "    data : dict\n",
    "        The key-value data stored in `filename`\n",
    "    See Also\n",
    "    --------\n",
    "    save_h5\n",
    "    '''\n",
    "    data = {}\n",
    "\n",
    "    def collect(k, v):\n",
    "        if isinstance(v, h5py.Dataset):\n",
    "            data[k] = v[()]\n",
    "\n",
    "    with h5py.File(filename, mode='r') as hf:\n",
    "        hf.visititems(collect)\n",
    "        \n",
    "    field = list(data.keys())[0]\n",
    "    field = 'mel/mag'\n",
    "    if trim is not None:\n",
    "        if len(data[field].shape)==3:\n",
    "            data[field] = data[field][:,:trim,:,np.newaxis]\n",
    "        else:\n",
    "            data[field] = data[field][:,:trim,:,:]\n",
    "        data['dynamic/tags'] = data['dynamic/tags'][:,:trim-1,:]\n",
    "    return data\n",
    "\n",
    "test_file = '/scratch/ci411/pcen/urbansed_feature_0/test/soundscape_test_unimodal478_0.h5'\n",
    "data = load_h5(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 215, 128, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['mel/mag'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = ['aga','altwhea','mel/mag', 'mel/valid']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
